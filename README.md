# Databricks_Dbt_Dlt Project

Designed and implemented a Medallion Architecture (Bronze, Silver, Gold layers) using Databricks and Delta Lake to ensure scalable and clean data processing pipelines, improving data reliability by 40%.

Built real-time data ingestion pipelines using Databricks Autoloader and PySpark Streaming, reducing batch latency and enabling near real-time analytics with 80% reduction in data lag.

Integrated Unity Catalog for centralized data governance, improving data discoverability and access control across multiple teams by 50%.

Developed parameterized and reusable workflows in Databricks with notebook control flows, improving pipeline flexibility and maintainability by 30%.

Created and orchestrated Delta Live Tables (DLT) pipelines using Lakehouse declarative pipelines, enabling automated and reliable ETL processing with built-in monitoring and error handling.

Implemented Slowly Changing Dimensions (SCD Type 1) automation for handling historical changes in dimension data, improving accuracy of historical reporting by 100%.

Modeled Fact and Dimension tables for a star schema-based Dimensional Data Model, improving data query performance by 60% in downstream reporting tools.

Used DBT (Data Build Tool) for modular and testable transformation logic, enabling version-controlled and production-ready SQL-based transformations.

